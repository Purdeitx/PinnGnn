{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3696e76159e6874e",
   "metadata": {},
   "source": [
    "# Tutorial: Deep Learning for PDEs - Comparing FEM, PINN, and GNN\n",
    "\n",
    "Welcome to this introductory tutorial on solving Partial Differential Equations (PDEs) using neural networks. This notebook is designed to guide you through the transition from traditional numerical methods (FEM) to modern deep learning approaches (PINNs and GNNs).\n",
    "\n",
    "## Introduction: The Poisson Equation\n",
    "\n",
    "The **Poisson equation** is a fundamental elliptic PDE used in physics and engineering to describe electrostatics, steady-state heat conduction, and fluid flow.\n",
    "\n",
    "### Mathematical Formulation\n",
    "We seek to find a scalar function $u(x, y)$ that satisfies:\n",
    "\n",
    "$$-\\Delta u(x, y) = f(x, y) \\quad \\forall (x, y) \\in \\Omega$$\n",
    "$$u(x, y) = 0 \\quad \\forall (x, y) \\in \\partial\\Omega$$\n",
    "\n",
    "Where:\n",
    "- $\\Omega$: The **spatial domain**, defined here as a unit square $[0, 1] \\times [0, 1]$.\n",
    "- **Time Domain**: Since this is a steady-state problem, the **simulation time** is $t=0$. The solution represents a static equilibrium. However, the models are built to anticipate future time-dependent variables $u(x, t)$.\n",
    "- $\\Delta$: The Laplace operator $\\nabla \\cdot \\nabla$, representing diffusion or curvature.\n",
    "- $f(x, y)$: The **source term**, which \"drives\" the solution (e.g., heat generation or charge density).\n",
    "\n",
    "### Modeling Approaches\n",
    "1. **Finite Element Method (FEM)**: A classical mesh-based technique that discretizes the space into simple elements (triangles).\n",
    "2. **Physics-Informed Neural Networks (PINN)**: A continuous, mesh-free approach that solves the PDE by minimizing residuals using Autograd.\n",
    "3. **Graph Neural Networks (GNN)**: A physics-aware modern approach that operates on the mesh graph, learning to map topology to solutions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbbcf4c63550af2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T07:39:49.144625100Z",
     "start_time": "2026-01-28T07:39:44.645704600Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Local modules\n",
    "from FEM.fem_solver import get_problem\n",
    "from config.physics import *\n",
    "from utils.plotting import *\n",
    "from utils.metrics import *\n",
    "from utils.geometry import *\n",
    "from utils.train_utils import * \n",
    "from utils.reporting import *\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    # Inicializa el contexto de CUDA explícitamente\n",
    "    torch.cuda.set_device(0)\n",
    "    torch.zeros(1).cuda()\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f93fbd",
   "metadata": {},
   "source": [
    "## 1. Defining the Domain: The Geometry Module\n",
    "\n",
    "Before solving any physics equation, we must define the **Domain** ($\\Omega$) where the phenomenon occurs. In this tutorial, we move away from \"hardcoded\" coordinates to a modular approach using `utils/geometry.py`.\n",
    "\n",
    "The **Geometry Object** acts as the single source of truth for our entire pipeline:\n",
    "* **Limits**: Defines the physical boundaries (e.g., a square from 0 to 1).\n",
    "* **Sampling**: It knows how to generate points inside the domain ($\\Omega$) and on its boundaries ($\\partial\\Omega$).\n",
    "\n",
    "By centralizing the geometry, if you decide to change the size of the plate or even its shape, you only need to modify the geometry initialization. This ensures that when we compare **FEM**, **PINN**, and **GNN**, we are all looking at the exact same \"piece of reality\".\n",
    "\n",
    "* **For FEM**: We will discretize this geometry into a **Mesh** (nodes and elements).\n",
    "* **For PINN**: We will treat it as a **Point Cloud** (sampling points inside and on the boundaries).\n",
    "* **For GNN**: We will treat it as a **Graph** (nodes as vertices, connections as edges)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d74447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the physical limits of our problem\n",
    "# This object will be the 'source of truth' for all methods\n",
    "geom = SquareGeometry(x_range=[0, 1], y_range=[0, 1], device=device)\n",
    "\n",
    "print(f\"Geometry initialized: Square domain from {geom.x_range} to {geom.y_range}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedbd798",
   "metadata": {},
   "source": [
    "## 2. The Numerical Baseline: Finite Element Method (FEM)\n",
    "\n",
    "To evaluate if a Neural Network is \"smart,\" we first need a reliable reference. The **Finite Element Method (FEM)** is the industry standard for solving PDEs like the Poisson equation.\n",
    "\n",
    "Unlike neural networks (PINNs), which are continuous, FEM is a **discrete method**. It transforms our continuous geometry into a collection of small, simple shapes (elements). \n",
    "\n",
    "### From Geometry to Mesh\n",
    "The FEM cannot work with raw ranges; it needs a **Mesh**. We take our `geom` object and discretize it.\n",
    "\n",
    "* **Nodes**: Specific coordinates where the solution $u$ is calculated.\n",
    "* **Elements**: Triangles or quads that connect the nodes.\n",
    "\n",
    "We use the `skfem` library to:\n",
    "1.  **Discretize** the domain.\n",
    "2.  **Assemble** the physics (Laplacian and Source terms).\n",
    "3.  **Solve** the resulting linear system to get the \"Ground Truth\" solution.\n",
    "\n",
    "This solution will be our gold standard to measure the accuracy of our PINN and GNN models later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c05f9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T07:39:57.094077200Z",
     "start_time": "2026-01-28T07:39:57.047663Z"
    }
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# STEP 1: DEFINE PHYSICS & MESH PARAMETERS\n",
    "# ==========================================\n",
    "# You can change 'sine' to 'const' to see how the solution changes\n",
    "\n",
    "nelem = 2               # Mesh resolution (nelem x nelem)\n",
    "porder = 2              # Polynomial order (Lin or cuad elements)\n",
    "mesh_type = 'quad'       # Mesh type: 'tri' or 'quad'\n",
    "source_type = 'sine'    # Source term type: 'constant' or 'sine'\n",
    "\n",
    "prob = get_problem(geometry=geom, nx=nelem, ny=nelem, porder=porder, source_type=source_type, mesh_type=mesh_type)\n",
    "u_exact = prob['u_exact']\n",
    "doflocs = prob['basis'].doflocs.T\n",
    "\n",
    "print(f\"Configuring FEM for {source_type} source term...\")\n",
    "phys = PoissonPhysics(source_type=source_type)\n",
    "print(f\"FEM Solution ready with {len(doflocs)} Degree of Freedom (DOF).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f85761",
   "metadata": {},
   "source": [
    "### 2.1 Mesh Configuration: From Continuous to Discrete\n",
    "\n",
    "Creating a mesh is the process of defining the resolution of our \"numerical microscope\". In this step, we configure how the geometry will be divided. \n",
    "\n",
    "The main parameters are:\n",
    "* **`nelem` (Resolution)**: The number of divisions along each axis. A higher number captures more detail but requires more memory.\n",
    "* **`porder` (Polynomial Order)**: Defines the complexity of the solution inside each element. \n",
    "    * *Order 1 (Linear)*: The solution is a straight line between nodes.\n",
    "    * *Order 2 (Quadratic)*: The solution can curve within the element, offering much higher precision.\n",
    "* **`mesh_type`**: We can use triangles (`tri`) for complex shapes or quadrilaterals (`quad`) for regular grids.\n",
    "\n",
    "We use a structured triangular mesh with the following parameters (aligned with the `CircleDeterministic.py` reference):\n",
    "\n",
    "| Parameter | Description | Default Value |\n",
    "| :--- | :--- | :--- |\n",
    "| `nelem` | Number of subdivisions along one side | 2 |\n",
    "| `porder` | Polynomial order of the basis functions | 2 (Quadratic) |\n",
    "| `source` | Source term type $f(x, y)$ | 'sine' |\n",
    "| `domain` | Spatial extent | $[0, 1]^2$ |\n",
    "| `t_sim`  | Simulation Time | Static ($t=0$) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7338d1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary from your notebook variables for the report\n",
    "FEM_CONFIG = {\n",
    "    'nelem': nelem,\n",
    "    'porder': porder,\n",
    "    'mesh_type': mesh_type,\n",
    "    'source_type': source_type\n",
    "}\n",
    "\n",
    "# Display summary\n",
    "print_config_summary(model=prob, config=FEM_CONFIG, model_type=\"FEM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13b3f87",
   "metadata": {},
   "source": [
    "### 2.2 Visualization: The Calculation Skeleton\n",
    "\n",
    "Visualization is key to ensuring our discretization is correct. Below, we plot the generated mesh. \n",
    "\n",
    "In this plot, you are seeing the **Topology** of the problem:\n",
    "* **Nodes (Vertices)**: The specific points where the FEM calculates the value of $u(x, y)$. In GNN terms, these will be our **Graph Nodes**.\n",
    "* **Edges/Elements**: The connections between nodes. In GNN terms, these define the **Graph Edges**.\n",
    "\n",
    "By passing our `geom` object to the `get_problem` function, the mesh is automatically scaled to the correct physical dimensions, ensuring that our \"skeleton\" perfectly matches the real-world domain we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340da039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# STEP 2: AUTOMATIC MESH VISUALIZATION\n",
    "# ==========================================\n",
    "# 1. Mesh configuration based on user parameters\n",
    "prob = get_problem(\n",
    "    geometry=geom,  \n",
    "    nx=nelem,\n",
    "    ny=nelem, \n",
    "    porder=porder, \n",
    "    mesh_type=mesh_type, \n",
    "    source_type=source_type,\n",
    ")\n",
    "\n",
    "# 2. Automatic mesh plotting function tri/cuad\n",
    "plot_fem_mesh(\n",
    "    prob, \n",
    "    title=f\"Mesh: {mesh_type.upper()} | Order: P{porder} | Res: {nelem}x{nelem}\"\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# =========================================\n",
    "print(f\"--- Ficha Técnica de la Malla ---\")\n",
    "print(f\"Estructura: {mesh_type.upper()} | Orden: P{porder}\")\n",
    "print(f\"Resolución: {nelem}x{nelem} elementos\")\n",
    "print(f\"Total DoFs (Nodos de cálculo): {len(prob['doflocs'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bdc18a",
   "metadata": {},
   "source": [
    "### 2.3 FEM Results: Establishing the Ground Truth\n",
    "\n",
    "After solving the system, we obtain the distribution of $u(x, y)$—for example, the steady-state temperature across the plate. \n",
    "\n",
    "This result is our **Reference Solution**. We will use it for:\n",
    "1.  **Direct Comparison**: To visually check if the PINN looks similar.\n",
    "2.  **Error Metrics**: To calculate the exact L2 error of the Neural Network.\n",
    "3.  **GNN Training**: In supervised learning, this FEM data will be the \"label\" or \"target\" that the Graph Neural Network will try to mimic.\n",
    "\n",
    "Pay attention to the color map: it represents the physical response of the system to the source term $f(x, y)$ we defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66b14e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T07:40:00.980504500Z",
     "start_time": "2026-01-28T07:40:00.852782300Z"
    }
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# STEP 3: SOLVE FEM and VISUALIZE GROUND TRUTH\n",
    "# ==========================================\n",
    "\n",
    "# Validation plots\n",
    "plot_fem_validation(prob, title=f\"Validation: {mesh_type} P{porder} (Res: {nelem})\")\n",
    "plt.show()\n",
    "\n",
    "# Error calculation\n",
    "l2_error = np.sqrt(np.mean((prob['u'] - prob['u_exact'])**2))\n",
    "print(f\"Mean Squared Error (Nodes): {l2_error:.6f}\")\n",
    "\n",
    "df_metrics, raw_metrics = calculate_fem_metrics(prob['u'], prob['u_exact'])\n",
    "print(f\"REPORT: {mesh_type} P{porder}\")\n",
    "display(df_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f40881",
   "metadata": {},
   "source": [
    "## 3. Physics-Informed Neural Networks (PINNs)\n",
    "\n",
    "The core idea of a PINN is to use a Neural Network as a **universal function approximator** $u_{\\theta}(x, y)$ that satisfies a differential equation. \n",
    "\n",
    "Unlike the FEM approach we just saw, PINNs are **mesh-free**. Instead of dividing the space into elements, we evaluate the \"physical health\" of the network at random points. If the network violates the laws of physics (the PDE), we penalize it through the Loss Function.\n",
    "\n",
    "The Neural Network ($u_\\theta$)\n",
    "Usually a simple **Multilayer Perceptron (MLP)**. It takes spatial coordinates $(x, y)$ as input and outputs the physical quantity of interest $u$.\n",
    "* **Input:** Coordinates $(x, y)$\n",
    "* **Output:** Predicted solution $\\hat{u}(x, y)$\n",
    "\n",
    "To guarantee a good solution, a PINN must balance two different goals:\n",
    "1.  **Physics Loss ($\\mathcal{L}_{pde}$)**: Does the network satisfy $-\\Delta u = f$ inside the domain?\n",
    "2.  **Boundary Loss ($\\mathcal{L}_{bc}$)**: Does the network respect the fixed values (e.g., $u=0$) at the edges?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc0d93a",
   "metadata": {},
   "source": [
    "### 3.1 The Engine: Automatic Differentiation\n",
    "\n",
    "The \"secret sauce\" that allows PINNs to solve PDEs is **Automatic Differentiation (AD)**. \n",
    "\n",
    "In traditional methods, we approximate derivatives using finite differences (slopes between mesh nodes). In PINNs, we use the same mechanism that trains standard AI (backpropagation) to calculate **exact analytical derivatives** of the network's output with respect to its input coordinates $(x, y)$.\n",
    "\n",
    "$$\\frac{\\partial u}{\\partial x} \\approx \\text{torch.autograd.grad}(u, x)$$\n",
    "\n",
    "This means our \"sensor\" for physics is continuous: we can calculate the Laplacian $\\Delta u$ at **any** coordinate, not just on a predefined grid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac41029",
   "metadata": {},
   "source": [
    "### 3.2 The Physics-Informed Loss Function\n",
    "\n",
    "Training a PINN is an optimization problem. We define a total loss function that guides the network toward the correct physical behavior:\n",
    "\n",
    "\n",
    "$$\\mathcal{L}_{total} = \\omega_{pde} \\mathcal{L}_{pde} + \\omega_{bc} \\mathcal{L}_{bc}$$\n",
    "\n",
    "* **Residual Loss ($\\mathcal{L}_{pde}$)**: We sample $N_{col}$ points inside the domain and minimize the residual. For the Poisson equation $-\\Delta u = f$ the residual is $r := -\\Delta u - f$. If $r=0$, the physics are perfectly satisfied. Thus, the loss function is defined to minimize the residual:\n",
    "  $$\\mathcal{L}_{PDE} = \\frac{1}{N_{col}} \\sum_{i=1}^{N_{col}} |-\\Delta u_\\theta(x_i) - f(x_i)|^2$$\n",
    "\n",
    "* **Boundary Loss ($\\mathcal{L}_{bc}$)**: We sample $N_{bc}$ points on the edges and force the network to match the boundary conditions (Dirichlet, Neumann, etc.): \n",
    "    $$\\mathcal{L}_{BC} = \\frac{1}{N_{bc}} \\sum_{j=1}^{N_{bc}} |u_\\theta(x_j) - 0|^2$$\n",
    "\n",
    "\n",
    "> **Student Note**: The weights $\\omega$ (like `lambda_bc` in our code) are crucial. Often, the boundary conditions are harder to learn than the interior physics, so we give them more \"importance\" during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf3c3ff",
   "metadata": {},
   "source": [
    "### 3.3 Collocation Points: Where the Physics Happens\n",
    "\n",
    "These are the points $x_i$ sampled randomly within the domain $\\Omega$ where we evaluate the PDE residual. Since the PINN is **mesh-free**, we can sample these points dynamically at every iteration or use a fixed set of points to guide the learning of the underlying physics.\n",
    "\n",
    "In this notebook, you can choose between two strategies for the points where the physics are evaluated:\n",
    "1.  **Random Collocation**: High flexibility. The network explores the domain freely.\n",
    "2.  **FEM-based Nodes**: We use the exact same nodes as the FEM mesh. This is useful for direct 1-to-1 comparisons and to see if the network can \"match\" the classical solution at specific points.\n",
    "\n",
    "**Why use random points?** Random sampling (especially if it's dynamic) prevents the network from \"memorizing\" the solution at specific locations, forcing it to learn the global underlying physical law."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db029be",
   "metadata": {},
   "source": [
    "### 3.4 Setting up the PINN Experiment\n",
    "\n",
    "#### Key Hyperparameters to watch:\n",
    "* **Max Epochs**: How many times the network will look at the domain.\n",
    "* **Learning Rate**: The size of the step the optimizer takes. If it's too high, the physics might \"explode\"; if it's too low, the network will never learn the PDE.\n",
    "* **Accelerator**: We use `auto` to detect if you have a GPU (CUDA) available, which significantly speeds up the Autograd process.\n",
    "\n",
    "#### Train, test & validation datasets.\n",
    "\n",
    "In a PINN, we don't have a \"fixed dataset\" of images or labels; we have an infinite number of coordinates $(x, y)$ in our domain. You might ask: If the physics are universal, shouldn't I use all possible points for training to get the perfect solution? The answer is No, and here is why: \n",
    "\n",
    "1. The Overfitting Trap in Physics\n",
    "A Neural Network is an incredibly flexible optimizer. If we only evaluate the PDE residual on a fixed, static set of training points, the network might find a \"mathematical shortcut\": it makes the residual zero exactly at those points while creating wild, non-physical oscillations (instabilities) in between them.\n",
    "\n",
    "Validation acts as an independent judge. It checks coordinates the network has never \"seen\" to ensure the solution is smooth and physically consistent everywhere. If the training error is low but the validation error is high, your network is \"memorizing\" the points instead of \"understanding\" the PDE.\n",
    "\n",
    "2. Stability and Convergence\n",
    "Physical systems are sensitive to boundaries and gradients. By monitoring a separate Validation Set, we can detect if the solution is becoming unstable or \"exploding\" before the training ends. It is our primary tool to ensure the numerical stability of the neural solver.\n",
    "\n",
    "In a standard AI project, you divide your \"data\" into three sets. In a PINN, we do the same, but instead of \"data samples\", we divide our **Geometry**:\n",
    "\n",
    "-  **Training Set (Collocation Points)**: The training set is the portion of the data used to fit the model parameters, allowing the algorithm to learn patterns from examples. In PINNs, these are the coordinates $(x, y)$ where the neural network will evaluate the PDE. The \"loss\" here is the physics residual. \n",
    "\n",
    "-  **Validation Set**: The validation set is used during development to tune hyperparameters, evaluate convergence and monitor overfitting. \n",
    "\n",
    "-  **Test Set (Ground Truth Comparison)**: If available, it measures real accuracy of the model. It is an independent and unbiased evaluation of the final model’s predictive performance.\n",
    "\n",
    "**Why is this relevant?** Because it allows us to prove that the PINN can generalize the solution to the entire domain, not just the points it \"saw\" during training.\n",
    "\n",
    "#### Data loaders \n",
    "\n",
    "The **DataLoader** is the pipe that connects our points to the GPU. \n",
    "\n",
    "* **Format**: In PINNs, the DataLoader delivers a dictionary or a tensor of **raw coordinates** $(x, y)$. \n",
    "* **Batches**: We don't feed all points at once. We send them in small groups (batches). For example, if we have 2000 points and a `batch_size=32`, the optimizer will update the physics 63 times per epoch.\n",
    "* **Format in this project**: Our `PinnDataset` organizes two types of inputs: `'pde'` (interior points) and `'bc'` (boundary points)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234abd9c",
   "metadata": {},
   "source": [
    "## 3.6. STEP 1: Initializing the PINN Strategy\n",
    "\n",
    "Now we move to the code. In this first step, we define the \"Brain\" (Hyperparameters) and the \"Eyes\" (Collocation points).\n",
    "\n",
    "**CRITICAL CONCEPT**: Notice that in the following cell, we are **NOT** loading any experimental data or solutions. We are only generating **coordinates**. \n",
    "* We define how many points we want in the interior (`n_train`).\n",
    "* We define how many points we want on the boundaries (`n_bc`).\n",
    "* We visualize them to ensure they cover the domain $\\Omega$ correctly.\n",
    "\n",
    "The PINN starts completely \"blind\". It only has a set of coordinates and a mathematical rule (the PDE) to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a642f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 1: DATA GENERATION & STRATEGY\n",
    "# ============================================================\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PINN.pinn_module import PinnDataset, ValDataset\n",
    "\n",
    "# --- Manual Control Dashboard ---\n",
    "use_fem_for_train = False  \n",
    "use_fem_for_test  = False   \n",
    "n_train, n_test, n_bc = 100, 100, 100\n",
    "lambda_bc = 10.0\n",
    "# --- Learning config ---\n",
    "hidden_dim = 50\n",
    "n_layers = 4\n",
    "lr = 1e-3\n",
    "max_epochs = 500\n",
    "batch_size = 25\n",
    "# --- Physics config ---\n",
    "source_type = 'sine'  # 'const' or 'sine'  \n",
    "my_physics = PoissonPhysics(source_type=source_type)\n",
    "\n",
    "# 1. Collocation points (Interior)\n",
    "if use_fem_for_train:\n",
    "    train_pts = torch.tensor(prob['doflocs'], dtype=torch.float32)\n",
    "    train_label = \"FEM Nodes\"\n",
    "else:\n",
    "    train_pts = geom.sample_interior(n_train)\n",
    "    train_label = \"Random Collocation\"\n",
    "\n",
    "# 2. Boundary points (BCs) - 2D Logics\n",
    "bc_coords = geom.sample_boundary(n_bc)\n",
    "side_mask = torch.randint(0, 4, (n_bc,))\n",
    "bc_coords[side_mask==0, 0] = 0.0; bc_coords[side_mask==1, 0] = 1.0\n",
    "bc_coords[side_mask==2, 1] = 0.0; bc_coords[side_mask==3, 1] = 1.0\n",
    "\n",
    "# 3. Test/Validation points\n",
    "if use_fem_for_test:\n",
    "    test_pts = torch.tensor(prob['doflocs'], dtype=torch.float32)\n",
    "    test_val = torch.tensor(prob['u'], dtype=torch.float32).view(-1, 1)\n",
    "    test_label = \"FEM Mesh\"\n",
    "else:\n",
    "    test_pts = geom.sample_interior(n_test)\n",
    "    test_val = None             # Random points\n",
    "    test_label = \"Random Test\"\n",
    "\n",
    "# 4. DataLoaders en PinnModule\n",
    "train_ds = PinnDataset(geometry=geom, pde_pts=train_pts, bc_pts=bc_coords)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "val_ds = ValDataset(geometry=geom, pts=test_pts, vals=test_val)\n",
    "val_loader = DataLoader(val_ds, batch_size=len(test_pts))\n",
    "\n",
    "# --- Visualización Adaptativa ---\n",
    "plot_pinn_strategy(\n",
    "    train_pts.detach().cpu().numpy(), \n",
    "    bc_coords.detach().cpu().numpy(), \n",
    "    test_pts.detach().cpu().numpy(), \n",
    "    title=f\"Strategy: {train_label} vs {test_label}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33b4761",
   "metadata": {},
   "source": [
    "### 3.7. STEP 2: Understanding the Physics-Informed Neural Network (PINN)\n",
    "\n",
    "To solve a PDE like the Poisson equation without a mesh, we use a Neural Network as a function approximator $u_{\\theta}(x, y)$. Unlike standard deep learning, we don't only minimize the error against data; we minimize the **residual of the PDE**.\n",
    "\n",
    "### The Core Components\n",
    "The project is divided into three main modules that you should understand:\n",
    "1. **`pinn_config.py` (Predefined configurations)**: Before building the model, we define a dictionary (or a configuration object) called pinn_config. Think of this as the control panel of the neural network.\n",
    "    - Architecture: Here we define how many neurons and layers the network has.\n",
    "    - Hyperparameters: We set the learning rate, the weight of the boundary conditions (lambda_bc), and the number of epochs.\n",
    "    - Physics Link: This is where we tell the PINN which physical law it must follow.\n",
    "\n",
    "2.  **`physics.py` (The Rules)**: The Mathematical Recipe. The PINN does not \"know\" physics by default. We must teach it the rules through a specific script: physics.py. This module is designed to be pluggable: you can swap the Poisson equation for any other differential equation (like Fourier or Cattaneo) as long as you provide the following three components:\n",
    "    - The Source Term ($f$). In our Poisson example, this is the heat source or charge distribution.\n",
    "       * Poisson case: $f(x, y) = 2\\pi^2 \\sin(\\pi x) \\sin(\\pi y)$.\n",
    "       * Implementation: The model evaluates this function at every coordinate $(x, y)$ to know what \"force\" is acting on the system.\n",
    "    - The Laplacian and the Residual ($\\mathcal{R}$). This is the heart of the PINN. We use Automatic Differentiation to calculate the Laplacian ($\\Delta u = \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}$).\n",
    "        * The Residual: We define $\\mathcal{R} = \\Delta u + f$\n",
    "        * The Goal: If the network is doing its job perfectly, $\\mathcal{R}$ should be zero everywhere. The training process is simply the network trying to minimize this residual.\n",
    "    - Boundary Conditions (BCs). Finally, we define the \"rules\" at the edges of our domain.\n",
    "        * Poisson case (Dirichlet): We force $u(x, y) = 0$ at the four walls of the square.\n",
    "        * Implementation: The physics module checks the network's prediction at the boundary coordinates and calculates the error against the target value (zero).\n",
    "\n",
    "3.  **`pinn_module.py` (The Engine)**: This contains the `PINNSystem` (built with PyTorch Lightning). It handles the training loops, the optimization (Adam/LBFGS), and the logging of metrics. This has been designed to be agnostic to the underlying physics and dimensionality, so no modifications should be required (in principle). At each training step it:\n",
    "    1. Takes a batch of coordinates from the DataLoader.\n",
    "    2. Asks the network for a prediction $u$.\n",
    "    3. Calls physics.py to calculate how much that prediction violates the PDE.\n",
    "    4. Computes the Total Loss (PDE + Boundary) and tells the optimizer how to adjust the neurons.\n",
    "\n",
    "---\n",
    "\n",
    "> **NOTE** The beauty of this modular structure is that the pinn_module.py (the engine) never changes. To solve a more complex problem, you only update physics.py:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe234a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 2: MODEL & TRAINER SETUP\n",
    "# ============================================================\n",
    "import torch\n",
    "import numpy as np\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from PINN.pinn_module import PINNSystem\n",
    "from config.physics import *\n",
    "from utils.train_utils import GradientMonitor, LossPlotterCallback\n",
    "\n",
    "# Update config\n",
    "PINN_CONFIG = {\n",
    "    'input_dim': train_pts.shape[-1],\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'num_layers': n_layers,\n",
    "    'lr': lr,\n",
    "    'epochs': max_epochs,\n",
    "    'lambda_bc': lambda_bc,\n",
    "    'use_fem_for_train': use_fem_for_train,\n",
    "    'use_fem_for_test': use_fem_for_test,\n",
    "    'n_collocation': len(train_pts), # Directly from the points we just created\n",
    "    'n_boundary': n_bc\n",
    "}\n",
    "\n",
    "# my_physics = PoissonPhysics(source_type='sine', scale=1.0)\n",
    "my_physics = PoissonGeneral(source_type='sine', scale=1.0)\n",
    "pinn = PINNSystem(**PINN_CONFIG, physics=my_physics)\n",
    "\n",
    "loss_plotter = LossPlotterCallback(\n",
    "    model_name=\"Poisson PINN\"\n",
    ")\n",
    "\n",
    "# Define Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=300, # Increased for PINNs as they can plateau\n",
    "        mode='min', \n",
    "        min_delta=1e-6,\n",
    "        verbose=True\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        monitor='val_loss', \n",
    "        save_top_k=1, \n",
    "        mode='min',\n",
    "        filename='best_pinn_{epoch}'\n",
    "    ),\n",
    "    GradientMonitor(verbose=True),\n",
    "    loss_plotter\n",
    "]\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=PINN_CONFIG['epochs'],\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    callbacks=callbacks,\n",
    "    log_every_n_steps=20\n",
    ")\n",
    "\n",
    "from utils.reporting import print_config_summary\n",
    "\n",
    "print_config_summary(pinn, PINN_CONFIG, model_type=\"PINN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd66be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"¿Cuda disponible?: {torch.cuda.is_available()}\")\n",
    "print(f\"¿Versión de CUDA de PyTorch?: {torch.version.cuda}\")\n",
    "print(f\"¿Dispositivos detectados?: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Nombre de la GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321ed920",
   "metadata": {},
   "source": [
    "### 3.8. Execution, Inference and Comparison\n",
    "\n",
    "Once the strategy is set and the model is assembled, we proceed to the final stage: training the \"neural solver\" and verifying if it has truly learned the laws of physics.\n",
    "\n",
    "1. Training\n",
    "\n",
    "    Before starting, we set torch.set_float32_matmul_precision('high'). This allows PyTorch to use the specialized Tensor Cores in modern GPUs (like NVIDIA Ampere or newer), significantly accelerating the training without compromising the precision required for PDEs.\n",
    "\n",
    "    When we call trainer.fit(), the PINN begins its optimization loop. In each epoch, the network adjusts its internal weights to minimize the PDE Residual (internal consistency) and the Boundary Loss (sticking to the edges).\n",
    "\n",
    "2. Inference\n",
    "    After training, we switch the model to eval() mode. This is crucial because it deactivates specific training layers (like Dropout) and ensures the model is ready for pure prediction.\n",
    "    - torch.no_grad(): We wrap the inference in this context to tell PyTorch not to calculate gradients, which saves memory and computing power.\n",
    "    - The Goal: We pass the exact coordinates of the FEM mesh to the PINN. This allows us to have a point-by-point prediction that we can directly compare with the classical solution.\n",
    "\n",
    "3. Visual comparison\n",
    "    A scientific result is only valid if it can be quantified. We use two types of visualizations:\n",
    "    - The Comparison Map: We plot the FEM solution side-by-side with the PINN prediction. At a glance, the physical patterns (the \"shape\" of the heat or pressure) should look identical.\n",
    "    - The Error Map ($|u_{FEM} - u_{PINN}|$): This is the \"Truth Map.\" It shows exactly where the neural network is struggling.\n",
    "\n",
    "> **NOTE**: If you see high error concentrated only at the boundaries, it means your lambda_bc was too low. If the error is high everywhere, the network might need more layers (n_layers) or more training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee88c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. High precision\n",
    "torch.set_float32_matmul_precision('high') \n",
    "\n",
    "# 2. Training loop\n",
    "trainer.fit(pinn, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "# 3. Inference\n",
    "pinn.eval()\n",
    "with torch.no_grad():\n",
    "    x_test = torch.tensor(prob['doflocs'], dtype=torch.float32).to(pinn.device)\n",
    "    u_pinn_pred = pinn(x_test).cpu().numpy().flatten()\n",
    "\n",
    "# 4. Results comparison \n",
    "fig_comp = plot_comparison_with_pinn(pinn, prob['u'], prob['doflocs'])\n",
    "plt.show()\n",
    "\n",
    "# 5. Error analysis\n",
    "fig_err = plot_error_analysis(u_fem=prob['u'], u_model=u_pinn_pred, model_name=\"PINN\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f124a4c1",
   "metadata": {},
   "source": [
    "## 4. Graph Neural Network (GNN) Resolution\n",
    "\n",
    "**What does it do?** A GNN operates on the mesh nodes. It uses **Message Passing** to gather information from neighbors and refine its prediction.\n",
    "**Approach:** In this step, the GNN is **Supervised**. It learns to map node coordinates to the FEM solution $U(x, t)$.\n",
    "**Connectivity:** Defined by the FEM mesh edges (adjacent neighbors)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b33288c",
   "metadata": {},
   "source": [
    "### GNN Configuration\n",
    "\n",
    "| Hyperparameter | Description | Value |\n",
    "| :--- | :--- | :--- |\n",
    "| `hidden_dim` | Width of Graph layers | 32 |\n",
    "| `num_layers` | Message passing steps | 3 |\n",
    "| `lr` | Learning rate | 1e-3 |\n",
    "| `epochs` | Training iterations | 500 |\n",
    "| `supervised` | Training against FEM targets | Yes |\n",
    "| `connectivity` | Graph edges type | Mesh-based |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecb1d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PINNGraphDataset(nelem=nelem, porder=porder)\n",
    "system_gnn = GNNSystem(hidden_dim=32, num_layers=3, lr=1e-3, supervised=True)\n",
    "trainer_gnn = pl.Trainer(max_epochs=500, accelerator='auto', enable_checkpointing=False, log_every_n_steps=1)\n",
    "trainer_gnn.fit(system_gnn, DataLoader(dataset, batch_size=1))\n",
    "\n",
    "u_gnn = system_gnn(dataset[0]['x'].to(system_gnn.device), dataset[0]['edge_index'].to(system_gnn.device)).detach().cpu().numpy().flatten()\n",
    "plot_comparison_with_fem(u_exact, u_gnn, doflocs, None, \"GNN\")\n",
    "plt.show()\n",
    "\n",
    "plot_error_analysis(u_exact, u_gnn, None, \"GNN\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be2f5a7",
   "metadata": {},
   "source": [
    "## 5. PINN vs GNN: Comparative Analysis\n",
    "\n",
    "### Recap\n",
    "- **PINN** learned the physics by looking at the interior PDE residuals. It is mesh-free but requires careful sampling.\n",
    "- **GNN** learned the behavior of the solution by observing the FEM ground truth. It is extremely fast at inference and understands the domain topology.\n",
    "\n",
    "| Feature | PINN | GNN (Supervised) |\n",
    "| :--- | :--- | :--- |\n",
    "| Need for Mesh | No | Yes |\n",
    "| Learning Target | PDE Residual | FEM Data |\n",
    "| Flexibility | High (any geometry) | Medium (restricted to graph) |\n",
    "| Accuracy | Smooth/Global | Local/Topology-aware |\n",
    "\n",
    "**Conclusion**: This tutorial demonstrates that both physics-informed and data-driven neural networks can approximate PDE solutions. In advanced cases, we combine these (PiGNN) to use the graph structure while enforcing physics residuals directly on the mesh nodes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PinnGnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
